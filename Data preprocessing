import re // to preform filtering of text.
import time. // to find out the training time
import tensorflow // to perform all the deep learning tasks

Importing dataset**

creating a dictonary that maps each line and its id
WHY -- when performing and preprocessing task , we need to be aware of where we are and where we want to go.
 "present state" - imported data   "state we want to go" - a dataset that contains an input set and an output set.
The inputs are the inputs that will be fed to the neural network. and output is the target state.
 
 * id2line ={}
 * for line in lines:
 *   _line = line.split(' +++$+++ ')
 *   if len(_line) == 5:        // incase the lines corpus has a field greater or less than 5 columns
 *      id2line[_line[0]] == _line[4]
 
 we now have a dictionary that has a key and a value corresponding to a line in a movie.
 
 We need to keep track of the conversations fpr the training data.
 we create a list of having all conversations
 
 *conversation_ids = []
 *for conversation in conversations[:-1]:
 *   _conversation = conversation.split(' +++$+++ ')[-1][-1][:-1].replace("'","").replace(" ","")
 *   _conversaton_ids.append(_conversation.split(','))
 
 we are going to get the questions and answers separately
 we need to remember that we need two huge datasets , one for questions and other for answers, with the same size of the datasets, 
 so we need to create two separate lists of questions and answers.
 answer to index i should be the answer to the question in index i .
 TO identify which conversations are questions and which are answers, we have the conversations_ids list which tells that the first
 id of an index is the Q and the second is the A.
 So we take alternate indices for Q and A, we create two separate lists, we get the key elements of the questions from the conversations_ids
 list and to get the line corresponding to the key we using our dictionary.
 
 *quesions = []
 *answers = []
 *for conversation in conversations_ids:   //looping over the list
 *    for i in range(len(conversation) - 1):   // looping over the dictionary to pick sentences to corresponding key ids
 *        questions.append(id2line[conversation[i]])
 *        answers.append(id2line[conversation[i+1]])
 
 // the clean_text function is used to remove all the complicated words and special symbols from the text using the 're' library,
 //now apply this function to the Q and A list
 
 *cleaned_questions = []
 *cleaned_ answers = []
 *for question in questions:
 *   cleaned_questions.append(clean_text(question))
 *for answer in answers:
 *   cleaned_answers.append(clean_text()answers))
 
 
 // now we have to remove the words that do not appear quite frequently, 
 // WHY -- we need to optimize our training, so we need only the essential words by removing the words that appear in the corpus for less
 // less than 5%. To do this we need to create a dictionary that maps each word to its count.Later by referring this dictionary we can 
 // remove the less frequent words.
 
 word2count = {}
 for question in clean_questions:
     for word in question.split():
        if word not in word2count:
          word2count[word] = 1
        else
          word2count[word] += 1
        
   //repeat for answer
   
   //creating  a threshold to filter out the words in the cleaned Q list and A list.
   //we essentially perform two tasks here. One is tokenisation and filtering of non frequent words.We are going to create two dictionaries
   // 1- maps all words of all questions to a unique integer, and 2- map all words of all answers to a unique integer and while creating 
   // the two dicionaries we will check if the count of the words satisfy the threshold value.
    
    threshold = 20
    questionswords2int = {}
    word_number = 0
    for word, count in word2count.items():
      if count >= threshold:
         questionswords2int[word] = word_number
         word_number += 1
         
    // Repeat the same to answer, by giving the words a unique interger, we are tokenising it.
    
   //adding the last tokens to our two tokenised dictionaries, they are SOS, EOS, PAD, OUT, which are used for encodind and decoding
   // SOS - start of string
   // EOS - end of string
   // PAD - for unequal question and answer lengths.
   // OUT - all the words that were filtered out in the recent dicitonaries.
   
   tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']
   for token in tokens:
    questionswords2int[token] = len(questionswords2int) + 1
    for token in tokens:
    answersords2int[token] = len(answerswords2int) + 1
    
    //We now need to create an inverse dicitonary that lets us to use it when we build the seq2seq model.
    
    answersint2word = {w_i: w for w, w_i in answerswords2int.items()}
    
    //now we add the EOS to the end of every answer in the clean_answers list.we are doing this as we need the EOS at the decoding layers of
    //of the seq2seq model
    
    for i in range(len(clean_answers)):
        clean_answers[i] += ' <EOS>'
   
 // Now we are going to translate all questions and answers to integers basing on the unique tokens we had assigned in the questionswords2int dictionary
 //we are going to loop through each Q in clean_questions and simultaneously through the dictionary quesrionswords2int to replace words 
 // with unique tokens
 
 questions_to_int = []
 for question in clean_questions:
   ints = []
   for word in question.split():
       if word not in questionswords2int:
          ints.append(questionswords2int['<OUT>'])
       else:
          ints.append(questionsword2int[word])
    
    //Repeat for answers
    
    //We are going to sort the Q and A by the length of the Questions
    //WHY -- because this will speed up the training and reduce the loss, because it will reduce the amount of padding during the training
    //we need the index of the question and the question itself -- by using enumerate function
    sorted_clean_questions = []
    sorted_clean_answers = []
    for length in range(1,25 + 1):
        for i in enumerate(questions_into_int):
          if len(i[1]) == length:
             sorted_clean_questions.append(questions_into_int[i[0]])
             sorted_clean_questions.append(answers_into_int[i[0]])
 
// THAT'S THE END OF DATA PREPROCESSING
